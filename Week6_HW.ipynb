{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e50351-15c7-4379-9369-cd41cd7ac272",
   "metadata": {},
   "source": [
    "# (Homework) Week 6 - DataScience Bootcamp Fall 2025\n",
    "\n",
    "All solution cells are replaced with `# TODO` placeholders so you can fill them in.\n",
    "\n",
    "**Name:** Nate Lin\\\n",
    "**Email:yl13340@nyu.edu**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ae2a1-9b4d-4b8e-87a8-fd32d8c107c8",
   "metadata": {},
   "source": [
    "### Problem 1: Dataset Splitting\n",
    "\n",
    "1. You have recordings of 44 phones from 100 people; each person records ~200 phones/day for 5 days.\n",
    "   - Design a valid training/validation/test split strategy that ensures the model generalizes to **new speakers**.\n",
    "\n",
    "2. You now receive an additional dataset of 10,000 phone recordings from **Kilian**, a single speaker.\n",
    "   - You must train a model that performs well **specifically for Kilian**, while also maintaining generalization.\n",
    "\n",
    "*Describe your proposed split strategy and reasoning.* (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1215f-525a-4fd4-8653-842279e505da",
   "metadata": {},
   "source": [
    "#Todo\n",
    "1. For the first problem, to design a model that generalizes to new speakers, the core principle is to create a speaker-disjoint split. Instead of randomly splitting the individual recordings, you must split the 100 speakers themselves into separate groups. A common strategy would be to assign 80 speakers to the training set, 10 speakers to the validation set, and the final 10 speakers to the test set. All recordings from the 80 \"training\" speakers would be used to train the model, while the recordings from the 10 \"validation\" speakers are used for hyperparameter tuning. This strategy ensures that the model is tested on voices it has never encountered, forcing it to learn generalizable features of speech rather than just memorizing the specific vocal characteristics of the speakers in the training data. If you were to split recordings randomly, the model would learn to recognize speakers who appear in both the training and test sets, leading to an artificially inflated and misleading performance score that would fail in a real-world scenario.\n",
    "2. When introducing the 10,000 recordings from Kilian, the goal changes to a hybrid one: achieving high performance specifically for Kilian (personalization) while maintaining good performance on new users (generalization). To accomplish this, you must manage two separate splitting strategies. First, you maintain the speaker-disjoint split for the original 100 people as described before (e.g., 80 general-train, 10 general-val, 10 general-test speakers). Second, you take Kilian's 10,000 recordings and split them randomly (or chronologically, which is often better) into training, validation, and test sets (e.g., 8,000 Kilian-train, 1,000 Kilian-val, 1,000 Kilian-test). The final combined training set will consist of both the General-Train (80 speakers) and Kilian-Train data. Critically, you must now track two separate validation and test scores. During training, you would monitor performance on both the General-Val set (to ensure generalization isn't degrading) and the Kilian-Val set (to ensure the model is learning his voice). Your final evaluation must report two distinct scores: one on the General-Test set (proving generalization) and one on the Kilian-Test set (proving personalization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b7930-1fef-4fd2-ac71-1467e8b165e8",
   "metadata": {},
   "source": [
    "### Problem 2: K-Nearest Neighbors\n",
    "\n",
    "1. **1-NN Classification:** Given dataset:\n",
    "\n",
    "   Positive: (1,2), (1,4), (5,4)\n",
    "\n",
    "   Negative: (3,1), (3,2)\n",
    "\n",
    "   Plot the 1-NN decision boundary and classify new points visually.\n",
    "\n",
    "2. **Feature Scaling:** Consider dataset:\n",
    "\n",
    "   Positive: (100,2), (100,4), (500,4)\n",
    "\n",
    "   Negative: (300,1), (300,2)\n",
    "\n",
    "   What would the 1-NN classify point (500,1) as **before and after scaling** to [0,1] per feature?\n",
    "\n",
    "3. **Handling Missing Values:** How can you modify K-NN to handle missing features in a test point?\n",
    "\n",
    "4. **High-dimensional Data:** Why can K-NN still work well for images even with thousands of pixels?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f66d2-4e36-4e30-8ef5-72d9b7986ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Data\n",
    "X_pos = np.array([[1,2],[1,4],[5,4]])\n",
    "X_neg = np.array([[3,1],[3,2]])\n",
    "X = np.vstack([X_pos, X_neg])\n",
    "y = np.array([1]*len(X_pos) + [0]*len(X_neg))  # 1=Positive, 0=Negative\n",
    "\n",
    "# Fit 1-NN\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Grid for decision boundary\n",
    "x_min, x_max = 0, 6\n",
    "y_min, y_max = 0, 6\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n",
    "                     np.linspace(y_min, y_max, 400))\n",
    "Z = knn.predict(np.c_[xx, yy])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6,6))\n",
    "cmap_light = ListedColormap(['#ffdddd', '#ddffdd'])\n",
    "cmap_bold  = ListedColormap(['#ff0000', '#00aa00'])\n",
    "\n",
    "plt.pcolormesh(xx, yy, Z.reshape(xx.shape), cmap=cmap_light, shading='auto')\n",
    "plt.scatter(X_neg[:,0], X_neg[:,1], c='red', edgecolor='k', s=100, label='Negative')\n",
    "plt.scatter(X_pos[:,0], X_pos[:,1], c='green', edgecolor='k', s=100, label='Positive')\n",
    "\n",
    "plt.xlim(x_min, x_max); plt.ylim(y_min, y_max)\n",
    "plt.legend()\n",
    "plt.title('1-NN Decision Boundary')\n",
    "plt.xlabel('x'); plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f766e-e313-4c28-a2af-b8a7985e3db7",
   "metadata": {},
   "source": [
    "### Problem 3: Part 1\n",
    "\n",
    "You are given a fully trained Perceptron model with weight vector **w**, along with training set **D_TR** and test set **D_TE**.\n",
    "\n",
    "1. Your co-worker suggests evaluating $h(x) = sign(w \\cdot x)$ for every $(x, y)$ in D_TR and D_TE. Does this help determine whether test error is higher than training error?\n",
    "2. Why is there no need to compute training error explicitly for the Perceptron algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca95dc-c37e-4f56-ab0a-9913bde3079f",
   "metadata": {},
   "source": [
    "#Todo\n",
    "1. Yes, it is the exact procedure for determining the training and test error. By evaluating h(x)=sign(w⋅x) for every point (x,y) in the training set D_TR, you can count the number of times h(x)/y. This count, divided by the total size of D_TR, gives you the training error. Similarly, performing the same calculation on D_TE gives you the test error. You can then directly compare these two error rates to determine if the test error is higher.\n",
    "\n",
    "2. There is no need to compute the full training error during the standard Perceptron Learning Algorithm (PLA) because the algorithm is mistake-driven. The PLA's update rule, w←w+yx, is only triggered when the current point (x,y) it is visiting is misclassified. The algorithm doesn't care if the total error is 50% or 5%; its only concern is finding a misclassified point and applying the update to correct it. The algorithm's termination condition (in the separable case) is the implicit computation of a zero training error. It stops only when it can make a full pass through all the data without finding a single mistake.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e8682-2b9f-4b15-a38e-2d3ec75591dc",
   "metadata": {},
   "source": [
    "### Problem 3: Two-point 2D Dataset (Part 2)\n",
    "\n",
    "Run the Perceptron algorithm **by hand or in code** on the following data:\n",
    "\n",
    "1. Positive class: (10, -2)\n",
    "2. Negative class: (12, 2)\n",
    "\n",
    "Start with $w_0 = (0, 0)$ and a learning rate of 1.\n",
    "\n",
    "- Compute how many updates are required until convergence.\n",
    "- Write down the sequence of $w_i$ vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd4597a-387e-4d5d-bbe3-f621afd13625",
   "metadata": {},
   "source": [
    "# Todo\n",
    "### Problem 3: Two-point 2D Dataset (Part 2)\n",
    "\n",
    "Algorithm Trace\n",
    "\n",
    "    Initialization: w0​=(0,0)\n",
    "\n",
    "Epoch 1\n",
    "\n",
    "    Check x1​: w0​⋅x1​=(0,0)⋅(10,−2)=0.\n",
    "\n",
    "        Misclassified (sign is 0, not +1). Update 1.\n",
    "\n",
    "        w1​=w0​+y1​x1​=(0,0)+1⋅(10,−2)=(10,−2)\n",
    "\n",
    "    Check x2​: w1​⋅x2​=(10,−2)⋅(12,2)=120−4=116.\n",
    "\n",
    "        Misclassified (sign is +1, not -1). Update 2.\n",
    "\n",
    "        w2​=w1​+y2​x2​=(10,−2)+(−1)⋅(12,2)=(10−12,−2−2)=(−2,−4)\n",
    "\n",
    "Epoch 2\n",
    "\n",
    "    Check x1​: w2​⋅x1​=(−2,−4)⋅(10,−2)=−20+8=−12.\n",
    "\n",
    "        Misclassified (sign is -1, not +1). Update 3.\n",
    "\n",
    "        w3​=w2​+y1​x1​=(−2,−4)+1⋅(10,−2)=(8,−6)\n",
    "\n",
    "    Check x2​: w3​⋅x2​=(8,−6)⋅(12,2)=96−12=84.\n",
    "\n",
    "        Misclassified (sign is +1, not -1). Update 4.\n",
    "\n",
    "        w4​=w3​+y2​x2​=(8,−6)+(−1)⋅(12,2)=(−4,−8)\n",
    "\n",
    "Epoch 3\n",
    "\n",
    "    Check x1​: w4​⋅x1​=(−4,−8)⋅(10,−2)=−40+16=−24.\n",
    "\n",
    "        Misclassified (sign is -1, not +1). Update 5.\n",
    "\n",
    "        w5​=w4​+y1​x1​=(−4,−8)+1⋅(10,−2)=(6,−10)\n",
    "\n",
    "    Check x2​: w5​⋅x2​=(6,−10)⋅(12,2)=72−20=52.\n",
    "\n",
    "        Misclassified (sign is +1, not -1). Update 6.\n",
    "\n",
    "        w6​=w5​+y2​x2​=(6,−10)+(−1)⋅(12,2)=(−6,−12)\n",
    "\n",
    "Epoch 4\n",
    "\n",
    "    Check x1​: w6​⋅x1​=(−6,−12)⋅(10,−2)=−60+24=−36.\n",
    "\n",
    "        Misclassified (sign is -1, not +1). Update 7.\n",
    "\n",
    "        w7​=w6​+y1​x1​=(−6,−12)+1⋅(10,−2)=(4,−14)\n",
    "\n",
    "    Check x2​: w7​⋅x2​=(4,−14)⋅(12,2)=48−28=20.\n",
    "\n",
    "        Misclassified (sign is +1, not -1). Update 8.\n",
    "\n",
    "        w8​=w7​+y2​x2​=(4,−14)+(−1)⋅(12,2)=(−8,−16)\n",
    "\n",
    "Epoch 5\n",
    "\n",
    "    Check x1​: w8​⋅x1​=(−8,−16)⋅(10,−2)=−80+32=−48.\n",
    "\n",
    "        Misclassified (sign is -1, not +1). Update 9.\n",
    "\n",
    "        w9​=w8​+y1​x1​=(−8,−16)+1⋅(10,−2)=(2,−18)\n",
    "\n",
    "    Check x2​: w9​⋅x2​=(2,−18)⋅(12,2)=24−36=−12.\n",
    "\n",
    "        Correctly classified (sign is -1). No update.\n",
    "\n",
    "Epoch 6\n",
    "\n",
    "    Check x1​: w9​⋅x1​=(2,−18)⋅(10,−2)=20+36=56.\n",
    "\n",
    "        Correctly classified (sign is +1). No update.\n",
    "\n",
    "    Check x2​: w9​⋅x2​=(2,−18)⋅(12,2)=24−36=−12.\n",
    "\n",
    "        Correctly classified (sign is -1). No update.\n",
    "\n",
    "The algorithm completes a full pass (Epoch 6) with no updates, so it converges.\n",
    "\n",
    "Results\n",
    "\n",
    "    Total Updates: 9\n",
    "\n",
    "    Sequence of wi​ vectors:\n",
    "\n",
    "        w0​=(0,0)\n",
    "\n",
    "        w1​=(10,−2)\n",
    "\n",
    "        w2​=(−2,−4)\n",
    "\n",
    "        w3​=(8,−6)\n",
    "\n",
    "        w4​=(−4,−8)\n",
    "\n",
    "        w5​=(6,−10)\n",
    "\n",
    "        w6​=(−6,−12)\n",
    "\n",
    "        w7​=(4,−14)\n",
    "\n",
    "        w8​=(−8,−16)\n",
    "\n",
    "        w9​=(2,−18) (Final converged vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba29c20-59b0-456f-994e-05897175596e",
   "metadata": {},
   "source": [
    "### Problem 4: Reconstructing the Weight Vector\n",
    "\n",
    "Given the log of Perceptron updates:\n",
    "\n",
    "| x | y | count |\n",
    "|---|---|--------|\n",
    "| (0, 0, 0, 0, 4) | +1 | 2 |\n",
    "| (0, 0, 6, 5, 0) | +1 | 1 |\n",
    "| (3, 0, 0, 0, 0) | -1 | 1 |\n",
    "| (0, 9, 3, 6, 0) | -1 | 1 |\n",
    "| (0, 1, 0, 2, 5) | -1 | 1 |\n",
    "\n",
    "Assume learning rate = 1 and initial weight $w_0 = (0, 0, 0, 0, 0)$.\n",
    "\n",
    "Compute the final weight vector after all updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb261e-d6ba-4ecd-a4f4-e9b6f5104079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo\n",
    "First row:\n",
    "w=(0,0,0,0,0)+2\\cdot (0,0,0,4,0)=(0,0,0,8,0)\n",
    "\n",
    "Second row:\n",
    "w=(0,0,0,8,0)+(0,6,5,0,0)=(0,6,5,8,0)\n",
    "\n",
    "Third row:\n",
    "w=(0,6,5,8,0)+(-1)\\cdot (3,0,0,0,0)=(-3,6,5,8,0)\n",
    "\n",
    "Fourth row:\n",
    "w=(-3,6,5,8,0)+(-1)\\cdot (0,9,3,6,0)=(-3,-3,2,2,0)\n",
    "\n",
    "Fifth row:\n",
    "w=(-3,-3,2,2,0)+(-1)\\cdot (0,1,0,2,5)=(-3,-4,2,0,-5)\n",
    "\n",
    "The final weight vector is: (-3, -4, 2, 0, -5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f23b69-9f59-46c6-8103-5783fadeb7c0",
   "metadata": {},
   "source": [
    "### Problem 5: Visualizing Perceptron Convergence\n",
    "\n",
    "Implement a Perceptron on a small 2D dataset with positive and negative examples.\n",
    "\n",
    "- Plot the data points.\n",
    "- After each update, visualize the decision boundary.\n",
    "- Show how it converges to a stable separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879a3a9-de75-40a0-a901-bd2009d2b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Small 2D dataset\n",
    "X = np.array([\n",
    "    [2, 3], [4, 5], [5, 2],   # Positive (+1)\n",
    "    [1, 1], [2, 1], [3, 2]    # Negative (-1)\n",
    "])\n",
    "y = np.array([1, 1, 1, -1, -1, -1])\n",
    "\n",
    "# Add bias term (x0 = 1)\n",
    "X_bias = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "# Initialize weights\n",
    "w = np.zeros(X_bias.shape[1])\n",
    "\n",
    "def plot_boundary(w, X, y, title):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    # Plot points\n",
    "    plt.scatter(X[y==1][:,0], X[y==1][:,1], c='green', marker='o', label='Positive')\n",
    "    plt.scatter(X[y==-1][:,0], X[y==-1][:,1], c='red', marker='x', label='Negative')\n",
    "    \n",
    "    # Decision boundary: w0 + w1*x + w2*y = 0\n",
    "    x_vals = np.linspace(0,6,100)\n",
    "    if w[2] != 0:\n",
    "        y_vals = -(w[0] + w[1]*x_vals)/w[2]\n",
    "        plt.plot(x_vals, y_vals, 'b--')\n",
    "    \n",
    "    plt.xlim(0,6); plt.ylim(0,6)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "update_count = 0\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(X_bias)):\n",
    "        if y[i] * np.dot(w, X_bias[i]) <= 0:  # Misclassified\n",
    "            w += y[i] * X_bias[i]             # Update rule\n",
    "            update_count += 1\n",
    "            plot_boundary(w, X, y, f'Update {update_count}: w={w}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
